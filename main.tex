\documentclass{resume} % Use the custom resume.cls style

\usepackage[left=0.4 in,top=0.4in,right=0.4 in,bottom=0.4in]{geometry} % Document margins
\newcommand{\tab}[1]{\hspace{.2667\textwidth}\rlap{#1}} 
\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\name{Abhishek Das} % Your name
% You can merge both of these into a single line, if you do not have a website.
\address{+91-8249698971 \\ abhishekdas69597@gmail.com \\ Hyderabad,India } 
\address{\href{https://github.com/dabhishek316}{github/dabhishek316} \\ \href{https://www.linkedin.com/in/abhishek-das-9859a31a2/}{in/abhishek-das/} \\ \href{https://medium.com/@abhishekdas69597}{medium/@abhishekdas69597}}  %

\begin{document}

%----------------------------------------------------------------------------------------
%	OBJECTIVE
%----------------------------------------------------------------------------------------

\begin{rSection}{OBJECTIVE}

{Microsoft Certified: Azure Data Fundamentals (DP-900) Data Engineer with 3+ years of hands-on experience in Python, SQL, Spark, and cutting-edge big data technologies. Proficient in Azure, Databricks Lakehouse, Data Lake, and Delta Lake, I specialize in building and optimizing robust data pipelines. Winner of SIH 2020. Eager to leverage my skills in a dynamic environment to drive impactful data solutions and support organizational growth through efficient data management and analysis.}


\end{rSection}
%----------------------------------------------------------------------------------------
%	EDUCATION SECTION
%----------------------------------------------------------------------------------------

\begin{rSection}{Education}

{\bf B.Tech Computer Science}, C.V. Raman College of Engineering   \hfill {2017 - 2021 }\\
CGPA : 8.7 
\end{rSection}
\begin{rSection}{EXPERIENCE}

\textbf{Consultant Data Engineer} \hfill Jul 2023 - Present\\
Deloitte In \hfill \textit{Hyderabad, India}
 \begin{itemize}
    \itemsep -3pt {} 
    \item \textbf{Engineered} and \textbf{deployed} a Data Exchange framework using PySpark and Python, enhancing data processing by 20\% and reducing data processing time by 15\% by integrating ServiceNow ticket creation via API from Azure Data Factory.
    \item \textbf{Initiated} and \textbf{optimized} a workflow to trigger jobs in Azure Databricks from Azure Data Factory using service principal authentication, implementing parallelism with multi-threading to reduce processing time by 20\%.
    \item {\textbf{Engineered}} and {\textbf{deployed}} the Query Generator framework, transforming XML inputs into SQL queries; ensured secure, efficient extract delivery, resulting in 40\% faster data retrieval and a 25\% reduction in manual query errors. . 

 \end{itemize}
 
\textbf{Data Engineer } \hfill Jul 2021 - Jul 2023\\
Cognizant \hfill \textit{Hyderabad, India}
 \begin{itemize}
    \itemsep -3pt {}
    \item \textbf{Processed} data using Spark DataFrames, including transformations (select, filter, map, groupBy, agg, join) and actions (show, write.save), improving data processing accuracy by 15\%.
    \item \textbf{Architected} and \textbf{executed} an event-triggered ADF pipeline, reducing data ingestion latency by 25\%. 
    \item \textbf{Developed} bronze and silver layer scripts to process data to different layers using PySpark and store in s3 buckets, resulting in a 30\% improvement in data processing efficiency. 
\end{itemize}

\end{rSection} 

%----------------------------------------------------------------------------------------
%	WORK EXPERIENCE SECTION
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
% TECHINICAL STRENGTHS	
%----------------------------------------------------------------------------------------
\begin{rSection}{SKILLS}

\begin{tabular}{ @{} >{\bfseries}l @{\hspace{6ex}} l }
Programming Language& : Python,Pyspark\\
Data Engineering& : Delta Lake, Spark SQL, Lakehouse, Cloud Optimization\\
Cloud Technologies& : Azure (ADF, ADLS, ASA), AWS (Resdhift, S3
Buckets)\\
 Databases/DWH&: SQL, Teradata SQL ,Postgres, T- SQL\\
 Tools&: GIT, Postman, PyCharm, VsCode, VM, SQL Server Management Studio\\
\end{tabular}\\
\end{rSection}



\begin{rSection}{PROJECTS}
\vspace{-1.25em}
\item \textbf{Amazon Data Analysis Project in Pyspark} {| \textit{Pyspark, Databricks, Python, Git }|} \href{https://github.com/dabhishek316/Amazon-Sales-Data-Analysis-Project-in-Pyspark}{Github}{
    \begin{itemize}
        \item The Amazon Data Analysis Project in PySpark leverages PySpark to analyze sales data and uncover key business insights. It identifies the \textbf{region and country with the highest profits}, determines the \textbf{maximum orders placed within 72 hours}, and finds the\textbf{ month with the highest average units sold}. Additionally, it calculates the \textbf{lowest and highest margin percentages} and highlights \textbf{the top 3 highest-value orders for specific item types}. This project showcases \textbf{PySpark's capabilities} in processing and analyzing large datasets for e-commerce optimization
    \end{itemize}
}\end{rSection} 


\end{document}
