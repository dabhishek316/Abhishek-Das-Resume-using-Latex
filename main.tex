\documentclass{resume} % Use the custom resume.cls style

\usepackage[left=0.4 in,top=0.4in,right=0.4 in,bottom=0.4in]{geometry} % Document margins
\newcommand{\tab}[1]{\hspace{.2667\textwidth}\rlap{#1}} 
\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\name{Abhishek Das} % Your name
% You can merge both of these into a single line, if you do not have a website.
\address{+91-8249698971 \\ abhishekdas69597@gmail.com \\ Hyderabad, India } 
\address{\href{https://github.com/dabhishek316}{Github/dabhishek316} \\ \href{https://www.linkedin.com/in/abhishek-das-9859a31a2/}{in/abhishek-das/} \\ \href{https://medium.com/@abhishekdas69597}{medium/@abhishekdas69597}}  %

\begin{document}

%----------------------------------------------------------------------------------------
%	OBJECTIVE
%----------------------------------------------------------------------------------------

\begin{rSection}{OBJECTIVE}

{Driven Data Engineer with 3+ years of experience in Python, SQL, Spark, and advanced big data technologies. Proficient in Azure, Databricks, Data Lake, and Delta Lake, specializing in building and optimizing robust data pipelines. Demonstrated success in enhancing data processing efficiency and reducing computation time. Eager to leverage skills to drive impactful data solutions and support organizational growth through efficient data management and analysis. .}


\end{rSection}
%----------------------------------------------------------------------------------------
%	EDUCATION SECTION
%----------------------------------------------------------------------------------------

\begin{rSection}{Education}

{\bf B.Tech Computer Science}, C.V. Raman College of Engineering   \hfill {2017 - 2021 }\\
CGPA : 8.7 
\end{rSection}
\begin{rSection}{EXPERIENCE}

\textbf{Consultant Data Engineer} \hfill Jul 2023 - Present\\
Deloitte In \hfill \textit{Hyderabad, India}
 \begin{itemize}
    \itemsep -3pt {} 

    \item Engineered and deployed a Data Exchange framework using PySpark and Python, enhancing data processing by 20\% and reducing data processing time by 15\% by integrating ServiceNow ticket creation via API from Azure Data Factory.
    \item Initiated and optimized a workflow to trigger jobs in Azure Databricks from Azure Data Factory using service principal authentication, implementing parallelism with multi-threading to reduce computation time by 20\%.
    \item Engineered and deployed the Query Generator framework, transforming XML inputs into SQL queries; ensured secure, efficient extract delivery, resulting in 40\% faster data retrieval and a 25\% reduction in manual query errors. 

 \end{itemize}
 
\textbf{Data Engineer } \hfill Jul 2021 - Jul 2023\\
Cognizant \hfill \textit{Hyderabad, India}
 \begin{itemize}
    \itemsep -3pt {}

    \item Processed data using Spark DataFrames, including transformations (select, filter, map, groupBy, agg, join) and actions (show, write.save), improving data operational accuracy by 15\%.
    \item Architected and executed an event-triggered ADF pipeline, reducing data ingestion latency by 25\%.
    \item Developed bronze and silver layer scripts to process data to different layers using PySpark and store in s3 buckets, resulting in a 30\% improvement in data computation efficiency. 
\end{itemize}

\end{rSection} 

%----------------------------------------------------------------------------------------
%	WORK EXPERIENCE SECTION
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
% TECHINICAL STRENGTHS	
%----------------------------------------------------------------------------------------
\begin{rSection}{SKILLS}

\begin{tabular}{ @{} >{\bfseries}l @{\hspace{6ex}} l }
Programming Language& : Python,Pyspark\\
Data Engineering& : Delta Lake, Spark SQL, Lakehouse, Cloud Optimization, Delta Live Table (DLT)\\
Cloud Technologies& : Azure (ADF, ADLS, ASA), AWS (Resdhift, S3
Buckets)\\
 Databases/DWH&: SQL, Teradata SQL ,Postgres, T- SQL\\
 Tools&: GIT, Postman, PyCharm, VsCode, VM, SQL Server Management Studio\\
\end{tabular}\\
\end{rSection}



\begin{rSection}{PROJECTS}
\vspace{-1.25em}
\item \textbf{Amazon Data Analysis Project in Pyspark} {| \textit{Pyspark, Databricks, Python, Git }|} \href{https://github.com/dabhishek316/Amazon-Sales-Data-Analysis-Project-in-Pyspark}{Github}{
    \begin{itemize}
        \item The Project leverages PySpark to analyze sales data and uncover key business insights. It identifies the \textbf{region and country with the highest profits}, determines the \textbf{maximum orders placed within 72 hours}, and finds the\textbf{ month with the highest average units sold}. Additionally, it calculates the \textbf{lowest and highest margin percentages} and highlights \textbf{the top 3 highest-value orders for specific item types}. This project showcases \textbf{PySpark's capabilities} in handling and analyzing large datasets for e-commerce optimization
    \end{itemize}
}\end{rSection} 


\end{document}
